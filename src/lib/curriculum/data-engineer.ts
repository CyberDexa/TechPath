import { Track } from "./types";

export const dataEngineerTrack: Track = {
  id: "data-engineer",
  title: "Data Engineer",
  description:
    "Design and build robust data pipelines, warehouses, and infrastructure. Master ETL/ELT, orchestration, big data, and cloud data platforms.",
  icon: "Database",
  color: "amber",
  category: "data-ai",
  modules: [
    {
      id: "de-intro",
      title: "Introduction to Data Engineering",
      description: "The data engineering role, lifecycle, and core responsibilities.",
      icon: "BookOpen",
      trackId: "data-engineer",
      order: 1,
      lessons: [
        {
          id: "de-intro-1",
          title: "What is Data Engineering?",
          description: "Role overview, responsibilities, and career paths.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-intro-2",
          title: "The Data Engineering Lifecycle",
          description: "Generation, storage, ingestion, transformation, and serving.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-intro-3",
          title: "Data Architecture Fundamentals",
          description: "Data lakes, warehouses, lakehouses, and modern data stack.",
          duration: "12 min",
          videoId: "qWru-b6m030",
          videoTitle: "Data Engineering Explained",
          videoChannel: "DataTalksClub",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Data Architecture Diagram",
        description: "Design a data architecture diagram for a sample company.",
        difficulty: "beginner",
        estimatedHours: 2,
        skills: ["Architecture", "Data Modeling"],
      },
    },
    {
      id: "de-programming",
      title: "Programming (Python & SQL)",
      description: "Python and SQL skills essential for data engineering.",
      icon: "Code",
      trackId: "data-engineer",
      order: 2,
      lessons: [
        {
          id: "de-prog-1",
          title: "Python for Data Engineering",
          description: "Data structures, file handling, and scripting for pipelines.",
          duration: "15 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write a Python script that reads a JSON file and converts it to CSV format.",
            validation: "json|csv|open|write",
          },
        },
        {
          id: "de-prog-2",
          title: "Advanced SQL for Engineers",
          description: "Window functions, CTEs, recursive queries, and performance.",
          duration: "15 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write a SQL query using a window function to rank customers by total spend.",
            validation: "RANK|ROW_NUMBER|OVER|PARTITION BY|window",
          },
        },
        {
          id: "de-prog-3",
          title: "Scripting & Automation",
          description: "Shell scripting and Python automation for data tasks.",
          duration: "12 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write a bash script that downloads a CSV from a URL, processes it with Python, and loads it into a SQLite database.",
            validation: "curl|wget|sqlite|python",
          },
        },
        {
          id: "de-prog-4",
          title: "Testing Data Code",
          description: "Unit tests, data validation, and great expectations.",
          duration: "10 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write pytest tests that validate a data transformation function.",
            validation: "pytest|assert|def test_",
          },
        },
        {
          id: "de-prog-5",
          title: "Version Control for Data Projects",
          description: "Git workflows, DVC, and managing data artifacts.",
          duration: "8 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Automated Data Processor",
        description: "Build a Python tool that automates data extraction, validation, and loading.",
        difficulty: "intermediate",
        estimatedHours: 4,
        skills: ["Python", "SQL", "Automation"],
      },
    },
    {
      id: "de-linux",
      title: "Linux & CLI",
      description: "Linux command line skills for managing data infrastructure.",
      icon: "Terminal",
      trackId: "data-engineer",
      order: 3,
      lessons: [
        {
          id: "de-linux-1",
          title: "Linux Fundamentals",
          description: "File system, permissions, users, and processes.",
          duration: "12 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Navigate the file system, create directories, set permissions, and list running processes.",
            validation: "mkdir|chmod|ls|ps",
          },
        },
        {
          id: "de-linux-2",
          title: "Text Processing Tools",
          description: "grep, awk, sed, and command-line data processing.",
          duration: "12 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Use awk and grep to extract and filter data from a CSV file.",
            validation: "awk|grep|sed|cut",
          },
        },
        {
          id: "de-linux-3",
          title: "Networking & SSH",
          description: "SSH, SCP, curl, and remote server management.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "CLI Data Pipeline",
        description: "Build a data pipeline using only Linux CLI tools.",
        difficulty: "beginner",
        estimatedHours: 3,
        skills: ["Linux", "CLI", "Text Processing"],
      },
    },
    {
      id: "de-lifecycle",
      title: "Data Engineering Lifecycle",
      description: "Deep dive into each stage of the data engineering lifecycle.",
      icon: "RefreshCw",
      trackId: "data-engineer",
      order: 4,
      lessons: [
        {
          id: "de-life-1",
          title: "Data Generation & Source Systems",
          description: "APIs, databases, IoT, logs — understanding source systems.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-life-2",
          title: "Data Ingestion Patterns",
          description: "Batch vs streaming, CDC, and ingestion tools.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-life-3",
          title: "Data Storage Strategies",
          description: "Row vs columnar, partitioning, and compression.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-life-4",
          title: "Data Transformation & Serving",
          description: "Transformation patterns and serving data to consumers.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Lifecycle Assessment",
        description: "Map a real company's data flow through each lifecycle stage.",
        difficulty: "beginner",
        estimatedHours: 3,
        skills: ["Data Architecture", "System Design"],
      },
    },
    {
      id: "de-databases",
      title: "Database Fundamentals",
      description: "Core database concepts, indexing, transactions, and ACID.",
      icon: "HardDrive",
      trackId: "data-engineer",
      order: 5,
      lessons: [
        {
          id: "de-db-1",
          title: "RDBMS Concepts",
          description: "Tables, schemas, normalization, and ACID properties.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-db-2",
          title: "Indexing & Query Optimization",
          description: "B-trees, hash indexes, and EXPLAIN plans.",
          duration: "15 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Create indexes on a table and use EXPLAIN ANALYZE to compare query performance.",
            validation: "CREATE INDEX|EXPLAIN|ANALYZE",
          },
        },
        {
          id: "de-db-3",
          title: "PostgreSQL Deep Dive",
          description: "Advanced PostgreSQL features for data engineering.",
          duration: "15 min",
          videoId: "qw--VYLpxG4",
          videoTitle: "PostgreSQL Tutorial",
          videoChannel: "freeCodeCamp",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Create a PostgreSQL table with partitioning and write a query using window functions.",
            validation: "PARTITION BY|RANGE|CREATE TABLE",
          },
        },
        {
          id: "de-db-4",
          title: "Transactions & Concurrency",
          description: "Isolation levels, locking, and MVCC.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-db-5",
          title: "Database Design Patterns",
          description: "Star schema, snowflake schema, and data vault.",
          duration: "12 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Schema Design for Analytics",
        description: "Design and implement a star schema database for an e-commerce analytics use case.",
        difficulty: "intermediate",
        estimatedHours: 5,
        skills: ["PostgreSQL", "Schema Design", "Indexing"],
      },
    },
    {
      id: "de-relational",
      title: "Relational Databases",
      description: "Work with PostgreSQL, MySQL, and relational database management.",
      icon: "Table2",
      trackId: "data-engineer",
      order: 6,
      lessons: [
        {
          id: "de-rel-1",
          title: "PostgreSQL for Data Engineering",
          description: "Extensions, COPY, JSONB, and advanced data types.",
          duration: "15 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Use PostgreSQL COPY to bulk load a CSV file and query JSONB columns.",
            validation: "COPY|JSONB|FROM|CSV",
          },
        },
        {
          id: "de-rel-2",
          title: "MySQL & MariaDB",
          description: "MySQL-specific features, replication, and migration.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-rel-3",
          title: "Connection Pooling & Management",
          description: "PgBouncer, connection pooling strategies.",
          duration: "8 min",
          hasTerminal: false,
        },
        {
          id: "de-rel-4",
          title: "Database Migration Strategies",
          description: "Schema migration tools, zero-downtime migrations.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Database Migration Script",
        description: "Write migration scripts to evolve a database schema without data loss.",
        difficulty: "intermediate",
        estimatedHours: 3,
        skills: ["PostgreSQL", "Migrations", "SQL"],
      },
    },
    {
      id: "de-nosql",
      title: "NoSQL Databases",
      description: "Document stores, key-value, columnar, and graph databases.",
      icon: "Layers",
      trackId: "data-engineer",
      order: 7,
      lessons: [
        {
          id: "de-nosql-1",
          title: "MongoDB & Document Stores",
          description: "CRUD operations, aggregation pipeline, and schema design.",
          duration: "15 min",
          videoId: "ExcRbA7fy_A",
          videoTitle: "MongoDB in 100 Seconds",
          videoChannel: "Fireship",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write MongoDB queries using insertMany, find with projection, and the aggregation pipeline.",
            validation: "insertMany|find|aggregate|\\$match|\\$group",
          },
        },
        {
          id: "de-nosql-2",
          title: "Redis & Key-Value Stores",
          description: "Caching, pub/sub, and Redis data structures.",
          duration: "10 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Use Redis commands to SET, GET, and use HSET for hash-based cached data.",
            validation: "SET|GET|HSET|redis",
          },
        },
        {
          id: "de-nosql-3",
          title: "Cassandra & Wide-Column Stores",
          description: "Cassandra data model, partitioning, and CQL.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-nosql-4",
          title: "Choosing SQL vs NoSQL",
          description: "Decision framework for database selection.",
          duration: "8 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Multi-Database Application",
        description: "Design a system that uses PostgreSQL for transactions and Redis for caching.",
        difficulty: "intermediate",
        estimatedHours: 4,
        skills: ["MongoDB", "Redis", "Database Selection"],
      },
    },
    {
      id: "de-warehouse",
      title: "Data Warehousing",
      description: "Design and manage data warehouses for analytics.",
      icon: "Warehouse",
      trackId: "data-engineer",
      order: 8,
      lessons: [
        {
          id: "de-wh-1",
          title: "Data Warehouse Concepts",
          description: "OLAP vs OLTP, dimensional modeling, and fact/dimension tables.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-wh-2",
          title: "Snowflake & BigQuery",
          description: "Cloud data warehouses — architecture and querying.",
          duration: "15 min",
          hasTerminal: false,
        },
        {
          id: "de-wh-3",
          title: "dbt (Data Build Tool)",
          description: "Transform data in your warehouse with dbt models and tests.",
          duration: "15 min",
          videoId: "M8oi7nSaWps",
          videoTitle: "dbt Tutorial",
          videoChannel: "DataTalksClub",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Create a dbt model file that transforms raw orders into a clean orders table with tests.",
            validation: "SELECT|FROM|ref\\(|source\\(",
          },
        },
        {
          id: "de-wh-4",
          title: "Slowly Changing Dimensions",
          description: "SCD Type 1, 2, and 3 patterns for historical data.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Data Warehouse Design",
        description: "Design and implement a star schema warehouse with dbt models.",
        difficulty: "advanced",
        estimatedHours: 6,
        skills: ["Data Warehousing", "dbt", "Dimensional Modeling"],
      },
    },
    {
      id: "de-pipelines",
      title: "Data Pipelines & ETL/ELT",
      description: "Build reliable, scalable data pipelines.",
      icon: "GitBranch",
      trackId: "data-engineer",
      order: 9,
      lessons: [
        {
          id: "de-pipe-1",
          title: "ETL vs ELT",
          description: "Understanding the difference and when to use each.",
          duration: "10 min",
          hasTerminal: false,
        },
        {
          id: "de-pipe-2",
          title: "Building ETL Pipelines in Python",
          description: "Extract, transform, and load data with Python.",
          duration: "15 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Build a Python ETL pipeline that extracts from an API, transforms data, and loads to SQLite.",
            validation: "extract|transform|load|sqlite|requests",
          },
        },
        {
          id: "de-pipe-3",
          title: "Data Quality & Validation",
          description: "Great Expectations, data contracts, and quality gates.",
          duration: "12 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write data validation checks using assertions for a DataFrame (null checks, range checks).",
            validation: "assert|isnull|between|validate",
          },
        },
        {
          id: "de-pipe-4",
          title: "Incremental Loading & CDC",
          description: "Change Data Capture and incremental pipeline patterns.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-pipe-5",
          title: "Error Handling & Monitoring",
          description: "Retry logic, dead letter queues, and pipeline observability.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Production ETL Pipeline",
        description: "Build a production-ready ETL pipeline with error handling and data validation.",
        difficulty: "advanced",
        estimatedHours: 6,
        skills: ["ETL", "Python", "Data Quality"],
      },
    },
    {
      id: "de-orchestration",
      title: "Workflow Orchestration",
      description: "Schedule and manage complex data workflows with Airflow and similar tools.",
      icon: "Network",
      trackId: "data-engineer",
      order: 10,
      lessons: [
        {
          id: "de-orch-1",
          title: "Apache Airflow Fundamentals",
          description: "DAGs, operators, sensors, and the Airflow UI.",
          duration: "15 min",
          videoId: "K9AnJ9_ZAXE",
          videoTitle: "Apache Airflow Tutorial",
          videoChannel: "freeCodeCamp",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write an Airflow DAG with 3 tasks: extract, transform, and load with dependencies.",
            validation: "DAG|PythonOperator|>>|task",
          },
        },
        {
          id: "de-orch-2",
          title: "Prefect & Modern Orchestrators",
          description: "Prefect flows, tasks, and deployment.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-orch-3",
          title: "Scheduling & Dependencies",
          description: "Cron expressions, task dependencies, and backfills.",
          duration: "10 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write a cron expression for running a pipeline every weekday at 6 AM UTC.",
            validation: "0 6 \\* \\* 1-5|cron",
          },
        },
        {
          id: "de-orch-4",
          title: "Monitoring & Alerting Pipelines",
          description: "SLAs, alerts, and observability for data pipelines.",
          duration: "10 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Orchestrated Data Pipeline",
        description: "Build an Airflow-orchestrated pipeline with multiple dependent tasks and error handling.",
        difficulty: "advanced",
        estimatedHours: 6,
        skills: ["Airflow", "Orchestration", "Pipeline Design"],
      },
    },
    {
      id: "de-big-data",
      title: "Big Data Processing",
      description: "Process large-scale data with Spark, Kafka, and distributed systems.",
      icon: "Flame",
      trackId: "data-engineer",
      order: 11,
      lessons: [
        {
          id: "de-big-1",
          title: "Apache Spark Fundamentals",
          description: "RDDs, DataFrames, and Spark SQL.",
          duration: "20 min",
          videoId: "_C8kWso4ne4",
          videoTitle: "PySpark Tutorial",
          videoChannel: "freeCodeCamp",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write PySpark code to read a CSV, filter rows, group by a column, and write output as Parquet.",
            validation: "SparkSession|read\\.csv|filter|groupBy|write\\.parquet",
          },
        },
        {
          id: "de-big-2",
          title: "Apache Kafka & Streaming",
          description: "Event streaming, producers, consumers, and topics.",
          duration: "15 min",
          videoId: "aj9CDZm0Glc",
          videoTitle: "Kafka in 100 Seconds",
          videoChannel: "Fireship",
          hasTerminal: false,
        },
        {
          id: "de-big-3",
          title: "Spark Streaming & Flink",
          description: "Real-time data processing with Spark Structured Streaming.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-big-4",
          title: "Data Formats (Parquet, Avro, ORC)",
          description: "Columnar formats, compression, and schema evolution.",
          duration: "10 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Convert a CSV to Parquet format using PyArrow and compare file sizes.",
            validation: "pyarrow|parquet|write_table|read_csv",
          },
        },
      ],
      project: {
        title: "Big Data Processing Pipeline",
        description: "Build a Spark pipeline that processes a large dataset and outputs aggregated results.",
        difficulty: "advanced",
        estimatedHours: 6,
        skills: ["Spark", "Big Data", "Parquet"],
      },
    },
    {
      id: "de-cloud",
      title: "Cloud Data Platforms",
      description: "Build data infrastructure on AWS, GCP, and Azure.",
      icon: "Cloud",
      trackId: "data-engineer",
      order: 12,
      lessons: [
        {
          id: "de-cloud-1",
          title: "AWS Data Services",
          description: "S3, Redshift, Glue, Athena, and Lambda for data.",
          duration: "15 min",
          hasTerminal: false,
        },
        {
          id: "de-cloud-2",
          title: "GCP Data Services",
          description: "BigQuery, Cloud Storage, Dataflow, and Pub/Sub.",
          duration: "12 min",
          hasTerminal: false,
        },
        {
          id: "de-cloud-3",
          title: "Infrastructure as Code",
          description: "Terraform for data infrastructure provisioning.",
          duration: "12 min",
          hasTerminal: true,
          terminalExercise: {
            instructions: "Write a Terraform configuration to create an S3 bucket with lifecycle policies.",
            validation: "resource.*aws_s3|lifecycle|terraform",
          },
        },
        {
          id: "de-cloud-4",
          title: "Cost Optimization",
          description: "Managing cloud data infrastructure costs effectively.",
          duration: "8 min",
          hasTerminal: false,
        },
      ],
      project: {
        title: "Cloud Data Platform",
        description: "Design and document a complete cloud data platform architecture.",
        difficulty: "advanced",
        estimatedHours: 5,
        skills: ["AWS", "GCP", "Terraform", "Architecture"],
      },
    },
  ],
};
